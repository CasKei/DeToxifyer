{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CasKei\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to normalise data (?)\n",
    "def normalise_x(inputX):\n",
    "  min_max_scaler = preprocessing.MinMaxScaler()\n",
    "  dfToNorm_scaled = min_max_scaler.fit_transform(inputX)\n",
    "  x = pd.DataFrame(dfToNorm_scaled)\n",
    "  return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Preprocessing texts in dataset \n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r'http\\S+', '', text) # Remove URLs\n",
    "        text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "        text = text.lower()\n",
    "        # Handle self-censored words and emojis as needed\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        return \"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Data\n",
      "   toxicity  severe_toxicity   obscene  sexual_explicit  identity_attack  \\\n",
      "0  0.373134         0.044776  0.089552         0.014925         0.000000   \n",
      "1  0.605263         0.013158  0.065789         0.013158         0.092105   \n",
      "2  0.666667         0.015873  0.031746         0.000000         0.047619   \n",
      "3  0.815789         0.065789  0.552632         0.592105         0.000000   \n",
      "4  0.550000         0.037500  0.337500         0.275000         0.037500   \n",
      "\n",
      "     insult    threat  \n",
      "0  0.343284  0.014925  \n",
      "1  0.565789  0.065789  \n",
      "2  0.666667  0.000000  \n",
      "3  0.684211  0.105263  \n",
      "4  0.487500  0.000000  \n",
      "is_toxic\n",
      "0    1835081\n",
      "1     164435\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read data and train test split\n",
    "data = pd.read_csv('all_data.csv')\n",
    "data['cleaned_text'] = data['comment_text'].apply(clean_text)\n",
    "# Specify the columns to consider for toxicity\n",
    "toxic_columns = ['toxicity', 'severe_toxicity', 'obscene', 'sexual_explicit', 'identity_attack', 'insult', 'threat']\n",
    "\n",
    "# # function to normalise data (?) \n",
    "# def normalise_x(inputX): \n",
    "#     min_max_scaler = preprocessing.MinMaxScaler() \n",
    "#     dfToNorm_scaled = min_max_scaler.fit_transform(inputX) \n",
    "#     x = pd.DataFrame(dfToNorm_scaled) \n",
    "#     return x\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Normalize the specified columns\n",
    "data[toxic_columns] = scaler.fit_transform(data[toxic_columns])\n",
    "\n",
    "# Print the normalized data\n",
    "print(\"Normalized Data\")\n",
    "print(data[toxic_columns].head())\n",
    "\n",
    "\n",
    "# Update the 'toxicity' label based on the values in the specified columns\n",
    "data['is_toxic'] = 0\n",
    "data['is_toxic'] = data.apply(lambda row: 1 if any(row[col] >= 0.5 for col in toxic_columns) else row['is_toxic'], axis=1)\n",
    "\n",
    "# Check the updated class distribution\n",
    "print(data['is_toxic'].value_counts())\n",
    "\n",
    "# Calculate the ratio of toxic to non-toxic samples\n",
    "toxic_count = data['is_toxic'].sum()\n",
    "non_toxic_count = len(data) - toxic_count\n",
    "ratio = non_toxic_count / toxic_count\n",
    "\n",
    "# If the ratio is still above a certain threshold, remove some non-toxic samples\n",
    "if ratio > 2:  # Adjust the threshold as per your requirement\n",
    "    non_toxic_data = data[data['is_toxic'] == 0]\n",
    "    toxic_data = data[data['is_toxic'] == 1]\n",
    "    \n",
    "    # Randomly select a subset of non-toxic samples to keep\n",
    "    num_samples_to_keep = int(toxic_count)  # Adjust the factor as per your requirement\n",
    "    non_toxic_data = non_toxic_data.sample(n=num_samples_to_keep, random_state=42)\n",
    "    \n",
    "    # Combine the toxic and selected non-toxic samples\n",
    "    data = pd.concat([toxic_data, non_toxic_data])\n",
    "    \n",
    "    # Shuffle the data\n",
    "    data = data.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         ... 0.6        0.64436297 1.        ]\n"
     ]
    }
   ],
   "source": [
    "a = np.sort(data['severe_toxicity'])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169088 159782\n"
     ]
    }
   ],
   "source": [
    "# make label binary 1 and 0 first for toxicity\n",
    "data['is_toxic'] = (data['toxicity'] >= 0.5).astype(int)\n",
    " \n",
    "count_zero = np.sum(data['is_toxic']  == 0) \n",
    "count_one = np.sum(data['is_toxic']  == 1)\n",
    "print(count_zero, count_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count_zero = np.sum(data['is_toxic']  == 0)\n",
    "count_one = np.sum(data['is_toxic']  == 1)\n",
    "\n",
    "print(count_zero, count_one)\n",
    "X = data['cleaned_text']\n",
    "y = data['is_toxic']\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CasKei\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    X_train.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    X_validation.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    X_test.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# cuda\n",
    "device = torch.device(\"cuda\")\n",
    "device\n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "scalar = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert lists to tensors\n",
    "\n",
    "train_seq = torch.tensor(scalar.fit_transform(tokens_train['input_ids'])).long().to(device)\n",
    "train_mask = torch.tensor(tokens_train['attention_mask']).to(device)\n",
    "train_y = torch.tensor(y_train.tolist()).to(device)\n",
    "\n",
    "val_seq = torch.tensor(scalar.fit_transform(tokens_val['input_ids'])).long().to(device)\n",
    "val_mask = torch.tensor(tokens_val['attention_mask']).to(device)\n",
    "val_y = torch.tensor(y_validation.tolist()).to(device)\n",
    "\n",
    "test_seq = torch.tensor(scalar.fit_transform(tokens_test['input_ids'])).long().to(device)\n",
    "test_mask = torch.tensor(tokens_test['attention_mask']).to(device)\n",
    "test_y = torch.tensor(y_test.tolist()).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "test_data = TensorDataset(test_seq, test_mask, test_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "\n",
    "# dataLoader for testidation set\n",
    "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        \n",
    "        self.bert = bert \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        \n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "      \n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "      \n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [0.97138595 1.03035097]\n",
      "number of weights:  2\n",
      "number of classes:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CasKei\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# optimizer from hugging face transformers\n",
    "# The compute_class_weight function from the sklearn.utils.class_weight module is used to compute the class weights with multiple parameters for the training labels.\n",
    "from transformers import AdamW\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),lr = 1e-4)\n",
    "\n",
    "#compute the class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train.tolist()), y=y_train.tolist())\n",
    "\n",
    "print('Class Weights:',class_weights)\n",
    "print('number of weights: ', len(class_weights))\n",
    "print('number of classes: ', len(np.unique(y_train.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "# push to GPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "  \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        \n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    " \n",
    "        sent_id, mask, labels = batch\n",
    "        \n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "      # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate(dataloader):\n",
    "    \n",
    "    print(\"\\nEvaluating...\")\n",
    "  \n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(dataloader):\n",
    "        \n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            \n",
    "            # Calculate elapsed time in minutes.\n",
    "            # elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 5\n",
      "  Batch    50  of  8,222.\n",
      "  Batch   100  of  8,222.\n",
      "  Batch   150  of  8,222.\n",
      "  Batch   200  of  8,222.\n",
      "  Batch   250  of  8,222.\n",
      "  Batch   300  of  8,222.\n",
      "  Batch   350  of  8,222.\n",
      "  Batch   400  of  8,222.\n",
      "  Batch   450  of  8,222.\n",
      "  Batch   500  of  8,222.\n",
      "  Batch   550  of  8,222.\n",
      "  Batch   600  of  8,222.\n",
      "  Batch   650  of  8,222.\n",
      "  Batch   700  of  8,222.\n",
      "  Batch   750  of  8,222.\n",
      "  Batch   800  of  8,222.\n",
      "  Batch   850  of  8,222.\n",
      "  Batch   900  of  8,222.\n",
      "  Batch   950  of  8,222.\n",
      "  Batch 1,000  of  8,222.\n",
      "  Batch 1,050  of  8,222.\n",
      "  Batch 1,100  of  8,222.\n",
      "  Batch 1,150  of  8,222.\n",
      "  Batch 1,200  of  8,222.\n",
      "  Batch 1,250  of  8,222.\n",
      "  Batch 1,300  of  8,222.\n",
      "  Batch 1,350  of  8,222.\n",
      "  Batch 1,400  of  8,222.\n",
      "  Batch 1,450  of  8,222.\n",
      "  Batch 1,500  of  8,222.\n",
      "  Batch 1,550  of  8,222.\n",
      "  Batch 1,600  of  8,222.\n",
      "  Batch 1,650  of  8,222.\n",
      "  Batch 1,700  of  8,222.\n",
      "  Batch 1,750  of  8,222.\n",
      "  Batch 1,800  of  8,222.\n",
      "  Batch 1,850  of  8,222.\n",
      "  Batch 1,900  of  8,222.\n",
      "  Batch 1,950  of  8,222.\n",
      "  Batch 2,000  of  8,222.\n",
      "  Batch 2,050  of  8,222.\n",
      "  Batch 2,100  of  8,222.\n",
      "  Batch 2,150  of  8,222.\n",
      "  Batch 2,200  of  8,222.\n",
      "  Batch 2,250  of  8,222.\n",
      "  Batch 2,300  of  8,222.\n",
      "  Batch 2,350  of  8,222.\n",
      "  Batch 2,400  of  8,222.\n",
      "  Batch 2,450  of  8,222.\n",
      "  Batch 2,500  of  8,222.\n",
      "  Batch 2,550  of  8,222.\n",
      "  Batch 2,600  of  8,222.\n",
      "  Batch 2,650  of  8,222.\n",
      "  Batch 2,700  of  8,222.\n",
      "  Batch 2,750  of  8,222.\n",
      "  Batch 2,800  of  8,222.\n",
      "  Batch 2,850  of  8,222.\n",
      "  Batch 2,900  of  8,222.\n",
      "  Batch 2,950  of  8,222.\n",
      "  Batch 3,000  of  8,222.\n",
      "  Batch 3,050  of  8,222.\n",
      "  Batch 3,100  of  8,222.\n",
      "  Batch 3,150  of  8,222.\n",
      "  Batch 3,200  of  8,222.\n",
      "  Batch 3,250  of  8,222.\n",
      "  Batch 3,300  of  8,222.\n",
      "  Batch 3,350  of  8,222.\n",
      "  Batch 3,400  of  8,222.\n",
      "  Batch 3,450  of  8,222.\n",
      "  Batch 3,500  of  8,222.\n",
      "  Batch 3,550  of  8,222.\n",
      "  Batch 3,600  of  8,222.\n",
      "  Batch 3,650  of  8,222.\n",
      "  Batch 3,700  of  8,222.\n",
      "  Batch 3,750  of  8,222.\n",
      "  Batch 3,800  of  8,222.\n",
      "  Batch 3,850  of  8,222.\n",
      "  Batch 3,900  of  8,222.\n",
      "  Batch 3,950  of  8,222.\n",
      "  Batch 4,000  of  8,222.\n",
      "  Batch 4,050  of  8,222.\n",
      "  Batch 4,100  of  8,222.\n",
      "  Batch 4,150  of  8,222.\n",
      "  Batch 4,200  of  8,222.\n",
      "  Batch 4,250  of  8,222.\n",
      "  Batch 4,300  of  8,222.\n",
      "  Batch 4,350  of  8,222.\n",
      "  Batch 4,400  of  8,222.\n",
      "  Batch 4,450  of  8,222.\n",
      "  Batch 4,500  of  8,222.\n",
      "  Batch 4,550  of  8,222.\n",
      "  Batch 4,600  of  8,222.\n",
      "  Batch 4,650  of  8,222.\n",
      "  Batch 4,700  of  8,222.\n",
      "  Batch 4,750  of  8,222.\n",
      "  Batch 4,800  of  8,222.\n",
      "  Batch 4,850  of  8,222.\n",
      "  Batch 4,900  of  8,222.\n",
      "  Batch 4,950  of  8,222.\n",
      "  Batch 5,000  of  8,222.\n",
      "  Batch 5,050  of  8,222.\n",
      "  Batch 5,100  of  8,222.\n",
      "  Batch 5,150  of  8,222.\n",
      "  Batch 5,200  of  8,222.\n",
      "  Batch 5,250  of  8,222.\n",
      "  Batch 5,300  of  8,222.\n",
      "  Batch 5,350  of  8,222.\n",
      "  Batch 5,400  of  8,222.\n",
      "  Batch 5,450  of  8,222.\n",
      "  Batch 5,500  of  8,222.\n",
      "  Batch 5,550  of  8,222.\n",
      "  Batch 5,600  of  8,222.\n",
      "  Batch 5,650  of  8,222.\n",
      "  Batch 5,700  of  8,222.\n",
      "  Batch 5,750  of  8,222.\n",
      "  Batch 5,800  of  8,222.\n",
      "  Batch 5,850  of  8,222.\n",
      "  Batch 5,900  of  8,222.\n",
      "  Batch 5,950  of  8,222.\n",
      "  Batch 6,000  of  8,222.\n",
      "  Batch 6,050  of  8,222.\n",
      "  Batch 6,100  of  8,222.\n",
      "  Batch 6,150  of  8,222.\n",
      "  Batch 6,200  of  8,222.\n",
      "  Batch 6,250  of  8,222.\n",
      "  Batch 6,300  of  8,222.\n",
      "  Batch 6,350  of  8,222.\n",
      "  Batch 6,400  of  8,222.\n",
      "  Batch 6,450  of  8,222.\n",
      "  Batch 6,500  of  8,222.\n",
      "  Batch 6,550  of  8,222.\n",
      "  Batch 6,600  of  8,222.\n",
      "  Batch 6,650  of  8,222.\n",
      "  Batch 6,700  of  8,222.\n",
      "  Batch 6,750  of  8,222.\n",
      "  Batch 6,800  of  8,222.\n",
      "  Batch 6,850  of  8,222.\n",
      "  Batch 6,900  of  8,222.\n",
      "  Batch 6,950  of  8,222.\n",
      "  Batch 7,000  of  8,222.\n",
      "  Batch 7,050  of  8,222.\n",
      "  Batch 7,100  of  8,222.\n",
      "  Batch 7,150  of  8,222.\n",
      "  Batch 7,200  of  8,222.\n",
      "  Batch 7,250  of  8,222.\n",
      "  Batch 7,300  of  8,222.\n",
      "  Batch 7,350  of  8,222.\n",
      "  Batch 7,400  of  8,222.\n",
      "  Batch 7,450  of  8,222.\n",
      "  Batch 7,500  of  8,222.\n",
      "  Batch 7,550  of  8,222.\n",
      "  Batch 7,600  of  8,222.\n",
      "  Batch 7,650  of  8,222.\n",
      "  Batch 7,700  of  8,222.\n",
      "  Batch 7,750  of  8,222.\n",
      "  Batch 7,800  of  8,222.\n",
      "  Batch 7,850  of  8,222.\n",
      "  Batch 7,900  of  8,222.\n",
      "  Batch 7,950  of  8,222.\n",
      "  Batch 8,000  of  8,222.\n",
      "  Batch 8,050  of  8,222.\n",
      "  Batch 8,100  of  8,222.\n",
      "  Batch 8,150  of  8,222.\n",
      "  Batch 8,200  of  8,222.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of  1,028.\n",
      "  Batch   100  of  1,028.\n",
      "  Batch   150  of  1,028.\n",
      "  Batch   200  of  1,028.\n",
      "  Batch   250  of  1,028.\n",
      "  Batch   300  of  1,028.\n",
      "  Batch   350  of  1,028.\n",
      "  Batch   400  of  1,028.\n",
      "  Batch   450  of  1,028.\n",
      "  Batch   500  of  1,028.\n",
      "  Batch   550  of  1,028.\n",
      "  Batch   600  of  1,028.\n",
      "  Batch   650  of  1,028.\n",
      "  Batch   700  of  1,028.\n",
      "  Batch   750  of  1,028.\n",
      "  Batch   800  of  1,028.\n",
      "  Batch   850  of  1,028.\n",
      "  Batch   900  of  1,028.\n",
      "  Batch   950  of  1,028.\n",
      "  Batch 1,000  of  1,028.\n",
      "\n",
      "Training Loss: 0.694\n",
      "Validation Loss: 0.693\n",
      "\n",
      " Epoch 2 / 5\n",
      "  Batch    50  of  8,222.\n",
      "  Batch   100  of  8,222.\n",
      "  Batch   150  of  8,222.\n",
      "  Batch   200  of  8,222.\n",
      "  Batch   250  of  8,222.\n",
      "  Batch   300  of  8,222.\n",
      "  Batch   350  of  8,222.\n",
      "  Batch   400  of  8,222.\n",
      "  Batch   450  of  8,222.\n",
      "  Batch   500  of  8,222.\n",
      "  Batch   550  of  8,222.\n",
      "  Batch   600  of  8,222.\n",
      "  Batch   650  of  8,222.\n",
      "  Batch   700  of  8,222.\n",
      "  Batch   750  of  8,222.\n",
      "  Batch   800  of  8,222.\n",
      "  Batch   850  of  8,222.\n",
      "  Batch   900  of  8,222.\n",
      "  Batch   950  of  8,222.\n",
      "  Batch 1,000  of  8,222.\n",
      "  Batch 1,050  of  8,222.\n",
      "  Batch 1,100  of  8,222.\n",
      "  Batch 1,150  of  8,222.\n",
      "  Batch 1,200  of  8,222.\n",
      "  Batch 1,250  of  8,222.\n",
      "  Batch 1,300  of  8,222.\n",
      "  Batch 1,350  of  8,222.\n",
      "  Batch 1,400  of  8,222.\n",
      "  Batch 1,450  of  8,222.\n",
      "  Batch 1,500  of  8,222.\n",
      "  Batch 1,550  of  8,222.\n",
      "  Batch 1,600  of  8,222.\n",
      "  Batch 1,650  of  8,222.\n",
      "  Batch 1,700  of  8,222.\n",
      "  Batch 1,750  of  8,222.\n",
      "  Batch 1,800  of  8,222.\n",
      "  Batch 1,850  of  8,222.\n",
      "  Batch 1,900  of  8,222.\n",
      "  Batch 1,950  of  8,222.\n",
      "  Batch 2,000  of  8,222.\n",
      "  Batch 2,050  of  8,222.\n",
      "  Batch 2,100  of  8,222.\n",
      "  Batch 2,150  of  8,222.\n",
      "  Batch 2,200  of  8,222.\n",
      "  Batch 2,250  of  8,222.\n",
      "  Batch 2,300  of  8,222.\n",
      "  Batch 2,350  of  8,222.\n",
      "  Batch 2,400  of  8,222.\n",
      "  Batch 2,450  of  8,222.\n",
      "  Batch 2,500  of  8,222.\n",
      "  Batch 2,550  of  8,222.\n",
      "  Batch 2,600  of  8,222.\n",
      "  Batch 2,650  of  8,222.\n",
      "  Batch 2,700  of  8,222.\n",
      "  Batch 2,750  of  8,222.\n",
      "  Batch 2,800  of  8,222.\n",
      "  Batch 2,850  of  8,222.\n",
      "  Batch 2,900  of  8,222.\n",
      "  Batch 2,950  of  8,222.\n",
      "  Batch 3,000  of  8,222.\n",
      "  Batch 3,050  of  8,222.\n",
      "  Batch 3,100  of  8,222.\n",
      "  Batch 3,150  of  8,222.\n",
      "  Batch 3,200  of  8,222.\n",
      "  Batch 3,250  of  8,222.\n",
      "  Batch 3,300  of  8,222.\n",
      "  Batch 3,350  of  8,222.\n",
      "  Batch 3,400  of  8,222.\n",
      "  Batch 3,450  of  8,222.\n",
      "  Batch 3,500  of  8,222.\n",
      "  Batch 3,550  of  8,222.\n",
      "  Batch 3,600  of  8,222.\n",
      "  Batch 3,650  of  8,222.\n",
      "  Batch 3,700  of  8,222.\n",
      "  Batch 3,750  of  8,222.\n",
      "  Batch 3,800  of  8,222.\n",
      "  Batch 3,850  of  8,222.\n",
      "  Batch 3,900  of  8,222.\n",
      "  Batch 3,950  of  8,222.\n",
      "  Batch 4,000  of  8,222.\n",
      "  Batch 4,050  of  8,222.\n",
      "  Batch 4,100  of  8,222.\n",
      "  Batch 4,150  of  8,222.\n",
      "  Batch 4,200  of  8,222.\n",
      "  Batch 4,250  of  8,222.\n",
      "  Batch 4,300  of  8,222.\n",
      "  Batch 4,350  of  8,222.\n",
      "  Batch 4,400  of  8,222.\n",
      "  Batch 4,450  of  8,222.\n",
      "  Batch 4,500  of  8,222.\n",
      "  Batch 4,550  of  8,222.\n",
      "  Batch 4,600  of  8,222.\n",
      "  Batch 4,650  of  8,222.\n",
      "  Batch 4,700  of  8,222.\n",
      "  Batch 4,750  of  8,222.\n",
      "  Batch 4,800  of  8,222.\n",
      "  Batch 4,850  of  8,222.\n",
      "  Batch 4,900  of  8,222.\n",
      "  Batch 4,950  of  8,222.\n",
      "  Batch 5,000  of  8,222.\n",
      "  Batch 5,050  of  8,222.\n",
      "  Batch 5,100  of  8,222.\n",
      "  Batch 5,150  of  8,222.\n",
      "  Batch 5,200  of  8,222.\n",
      "  Batch 5,250  of  8,222.\n",
      "  Batch 5,300  of  8,222.\n",
      "  Batch 5,350  of  8,222.\n",
      "  Batch 5,400  of  8,222.\n",
      "  Batch 5,450  of  8,222.\n",
      "  Batch 5,500  of  8,222.\n",
      "  Batch 5,550  of  8,222.\n",
      "  Batch 5,600  of  8,222.\n",
      "  Batch 5,650  of  8,222.\n",
      "  Batch 5,700  of  8,222.\n",
      "  Batch 5,750  of  8,222.\n",
      "  Batch 5,800  of  8,222.\n",
      "  Batch 5,850  of  8,222.\n",
      "  Batch 5,900  of  8,222.\n",
      "  Batch 5,950  of  8,222.\n",
      "  Batch 6,000  of  8,222.\n",
      "  Batch 6,050  of  8,222.\n",
      "  Batch 6,100  of  8,222.\n",
      "  Batch 6,150  of  8,222.\n",
      "  Batch 6,200  of  8,222.\n",
      "  Batch 6,250  of  8,222.\n",
      "  Batch 6,300  of  8,222.\n",
      "  Batch 6,350  of  8,222.\n",
      "  Batch 6,400  of  8,222.\n",
      "  Batch 6,450  of  8,222.\n",
      "  Batch 6,500  of  8,222.\n",
      "  Batch 6,550  of  8,222.\n",
      "  Batch 6,600  of  8,222.\n",
      "  Batch 6,650  of  8,222.\n",
      "  Batch 6,700  of  8,222.\n",
      "  Batch 6,750  of  8,222.\n",
      "  Batch 6,800  of  8,222.\n",
      "  Batch 6,850  of  8,222.\n",
      "  Batch 6,900  of  8,222.\n",
      "  Batch 6,950  of  8,222.\n",
      "  Batch 7,000  of  8,222.\n",
      "  Batch 7,050  of  8,222.\n",
      "  Batch 7,100  of  8,222.\n",
      "  Batch 7,150  of  8,222.\n",
      "  Batch 7,200  of  8,222.\n",
      "  Batch 7,250  of  8,222.\n",
      "  Batch 7,300  of  8,222.\n",
      "  Batch 7,350  of  8,222.\n",
      "  Batch 7,400  of  8,222.\n",
      "  Batch 7,450  of  8,222.\n",
      "  Batch 7,500  of  8,222.\n",
      "  Batch 7,550  of  8,222.\n",
      "  Batch 7,600  of  8,222.\n",
      "  Batch 7,650  of  8,222.\n",
      "  Batch 7,700  of  8,222.\n",
      "  Batch 7,750  of  8,222.\n",
      "  Batch 7,800  of  8,222.\n",
      "  Batch 7,850  of  8,222.\n",
      "  Batch 7,900  of  8,222.\n",
      "  Batch 7,950  of  8,222.\n",
      "  Batch 8,000  of  8,222.\n",
      "  Batch 8,050  of  8,222.\n",
      "  Batch 8,100  of  8,222.\n",
      "  Batch 8,150  of  8,222.\n",
      "  Batch 8,200  of  8,222.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of  1,028.\n",
      "  Batch   100  of  1,028.\n",
      "  Batch   150  of  1,028.\n",
      "  Batch   200  of  1,028.\n",
      "  Batch   250  of  1,028.\n",
      "  Batch   300  of  1,028.\n",
      "  Batch   350  of  1,028.\n",
      "  Batch   400  of  1,028.\n",
      "  Batch   450  of  1,028.\n",
      "  Batch   500  of  1,028.\n",
      "  Batch   550  of  1,028.\n",
      "  Batch   600  of  1,028.\n",
      "  Batch   650  of  1,028.\n",
      "  Batch   700  of  1,028.\n",
      "  Batch   750  of  1,028.\n",
      "  Batch   800  of  1,028.\n",
      "  Batch   850  of  1,028.\n",
      "  Batch   900  of  1,028.\n",
      "  Batch   950  of  1,028.\n",
      "  Batch 1,000  of  1,028.\n",
      "\n",
      "Training Loss: 0.693\n",
      "Validation Loss: 0.693\n",
      "\n",
      " Epoch 3 / 5\n",
      "  Batch    50  of  8,222.\n",
      "  Batch   100  of  8,222.\n",
      "  Batch   150  of  8,222.\n",
      "  Batch   200  of  8,222.\n",
      "  Batch   250  of  8,222.\n",
      "  Batch   300  of  8,222.\n",
      "  Batch   350  of  8,222.\n",
      "  Batch   400  of  8,222.\n",
      "  Batch   450  of  8,222.\n",
      "  Batch   500  of  8,222.\n",
      "  Batch   550  of  8,222.\n",
      "  Batch   600  of  8,222.\n",
      "  Batch   650  of  8,222.\n",
      "  Batch   700  of  8,222.\n",
      "  Batch   750  of  8,222.\n",
      "  Batch   800  of  8,222.\n",
      "  Batch   850  of  8,222.\n",
      "  Batch   900  of  8,222.\n",
      "  Batch   950  of  8,222.\n",
      "  Batch 1,000  of  8,222.\n",
      "  Batch 1,050  of  8,222.\n",
      "  Batch 1,100  of  8,222.\n",
      "  Batch 1,150  of  8,222.\n",
      "  Batch 1,200  of  8,222.\n",
      "  Batch 1,250  of  8,222.\n",
      "  Batch 1,300  of  8,222.\n",
      "  Batch 1,350  of  8,222.\n",
      "  Batch 1,400  of  8,222.\n",
      "  Batch 1,450  of  8,222.\n",
      "  Batch 1,500  of  8,222.\n",
      "  Batch 1,550  of  8,222.\n",
      "  Batch 1,600  of  8,222.\n",
      "  Batch 1,650  of  8,222.\n",
      "  Batch 1,700  of  8,222.\n",
      "  Batch 1,750  of  8,222.\n",
      "  Batch 1,800  of  8,222.\n",
      "  Batch 1,850  of  8,222.\n",
      "  Batch 1,900  of  8,222.\n",
      "  Batch 1,950  of  8,222.\n",
      "  Batch 2,000  of  8,222.\n",
      "  Batch 2,050  of  8,222.\n",
      "  Batch 2,100  of  8,222.\n",
      "  Batch 2,150  of  8,222.\n",
      "  Batch 2,200  of  8,222.\n",
      "  Batch 2,250  of  8,222.\n",
      "  Batch 2,300  of  8,222.\n",
      "  Batch 2,350  of  8,222.\n",
      "  Batch 2,400  of  8,222.\n",
      "  Batch 2,450  of  8,222.\n",
      "  Batch 2,500  of  8,222.\n",
      "  Batch 2,550  of  8,222.\n",
      "  Batch 2,600  of  8,222.\n",
      "  Batch 2,650  of  8,222.\n",
      "  Batch 2,700  of  8,222.\n",
      "  Batch 2,750  of  8,222.\n",
      "  Batch 2,800  of  8,222.\n",
      "  Batch 2,850  of  8,222.\n",
      "  Batch 2,900  of  8,222.\n",
      "  Batch 2,950  of  8,222.\n",
      "  Batch 3,000  of  8,222.\n",
      "  Batch 3,050  of  8,222.\n",
      "  Batch 3,100  of  8,222.\n",
      "  Batch 3,150  of  8,222.\n",
      "  Batch 3,200  of  8,222.\n",
      "  Batch 3,250  of  8,222.\n",
      "  Batch 3,300  of  8,222.\n",
      "  Batch 3,350  of  8,222.\n",
      "  Batch 3,400  of  8,222.\n",
      "  Batch 3,450  of  8,222.\n",
      "  Batch 3,500  of  8,222.\n",
      "  Batch 3,550  of  8,222.\n",
      "  Batch 3,600  of  8,222.\n",
      "  Batch 3,650  of  8,222.\n",
      "  Batch 3,700  of  8,222.\n",
      "  Batch 3,750  of  8,222.\n",
      "  Batch 3,800  of  8,222.\n",
      "  Batch 3,850  of  8,222.\n",
      "  Batch 3,900  of  8,222.\n",
      "  Batch 3,950  of  8,222.\n",
      "  Batch 4,000  of  8,222.\n",
      "  Batch 4,050  of  8,222.\n",
      "  Batch 4,100  of  8,222.\n",
      "  Batch 4,150  of  8,222.\n",
      "  Batch 4,200  of  8,222.\n",
      "  Batch 4,250  of  8,222.\n",
      "  Batch 4,300  of  8,222.\n",
      "  Batch 4,350  of  8,222.\n",
      "  Batch 4,400  of  8,222.\n",
      "  Batch 4,450  of  8,222.\n",
      "  Batch 4,500  of  8,222.\n",
      "  Batch 4,550  of  8,222.\n",
      "  Batch 4,600  of  8,222.\n",
      "  Batch 4,650  of  8,222.\n",
      "  Batch 4,700  of  8,222.\n",
      "  Batch 4,750  of  8,222.\n",
      "  Batch 4,800  of  8,222.\n",
      "  Batch 4,850  of  8,222.\n",
      "  Batch 4,900  of  8,222.\n",
      "  Batch 4,950  of  8,222.\n",
      "  Batch 5,000  of  8,222.\n",
      "  Batch 5,050  of  8,222.\n",
      "  Batch 5,100  of  8,222.\n",
      "  Batch 5,150  of  8,222.\n",
      "  Batch 5,200  of  8,222.\n",
      "  Batch 5,250  of  8,222.\n",
      "  Batch 5,300  of  8,222.\n",
      "  Batch 5,350  of  8,222.\n",
      "  Batch 5,400  of  8,222.\n",
      "  Batch 5,450  of  8,222.\n",
      "  Batch 5,500  of  8,222.\n",
      "  Batch 5,550  of  8,222.\n",
      "  Batch 5,600  of  8,222.\n",
      "  Batch 5,650  of  8,222.\n",
      "  Batch 5,700  of  8,222.\n",
      "  Batch 5,750  of  8,222.\n",
      "  Batch 5,800  of  8,222.\n",
      "  Batch 5,850  of  8,222.\n",
      "  Batch 5,900  of  8,222.\n",
      "  Batch 5,950  of  8,222.\n",
      "  Batch 6,000  of  8,222.\n",
      "  Batch 6,050  of  8,222.\n",
      "  Batch 6,100  of  8,222.\n",
      "  Batch 6,150  of  8,222.\n",
      "  Batch 6,200  of  8,222.\n",
      "  Batch 6,250  of  8,222.\n",
      "  Batch 6,300  of  8,222.\n",
      "  Batch 6,350  of  8,222.\n",
      "  Batch 6,400  of  8,222.\n",
      "  Batch 6,450  of  8,222.\n",
      "  Batch 6,500  of  8,222.\n",
      "  Batch 6,550  of  8,222.\n",
      "  Batch 6,600  of  8,222.\n",
      "  Batch 6,650  of  8,222.\n",
      "  Batch 6,700  of  8,222.\n",
      "  Batch 6,750  of  8,222.\n",
      "  Batch 6,800  of  8,222.\n",
      "  Batch 6,850  of  8,222.\n",
      "  Batch 6,900  of  8,222.\n",
      "  Batch 6,950  of  8,222.\n",
      "  Batch 7,000  of  8,222.\n",
      "  Batch 7,050  of  8,222.\n",
      "  Batch 7,100  of  8,222.\n",
      "  Batch 7,150  of  8,222.\n",
      "  Batch 7,200  of  8,222.\n",
      "  Batch 7,250  of  8,222.\n",
      "  Batch 7,300  of  8,222.\n",
      "  Batch 7,350  of  8,222.\n",
      "  Batch 7,400  of  8,222.\n",
      "  Batch 7,450  of  8,222.\n",
      "  Batch 7,500  of  8,222.\n",
      "  Batch 7,550  of  8,222.\n",
      "  Batch 7,600  of  8,222.\n",
      "  Batch 7,650  of  8,222.\n",
      "  Batch 7,700  of  8,222.\n",
      "  Batch 7,750  of  8,222.\n",
      "  Batch 7,800  of  8,222.\n",
      "  Batch 7,850  of  8,222.\n",
      "  Batch 7,900  of  8,222.\n",
      "  Batch 7,950  of  8,222.\n",
      "  Batch 8,000  of  8,222.\n",
      "  Batch 8,050  of  8,222.\n",
      "  Batch 8,100  of  8,222.\n",
      "  Batch 8,150  of  8,222.\n",
      "  Batch 8,200  of  8,222.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of  1,028.\n",
      "  Batch   100  of  1,028.\n",
      "  Batch   150  of  1,028.\n",
      "  Batch   200  of  1,028.\n",
      "  Batch   250  of  1,028.\n",
      "  Batch   300  of  1,028.\n",
      "  Batch   350  of  1,028.\n",
      "  Batch   400  of  1,028.\n",
      "  Batch   450  of  1,028.\n",
      "  Batch   500  of  1,028.\n",
      "  Batch   550  of  1,028.\n",
      "  Batch   600  of  1,028.\n",
      "  Batch   650  of  1,028.\n",
      "  Batch   700  of  1,028.\n",
      "  Batch   750  of  1,028.\n",
      "  Batch   800  of  1,028.\n",
      "  Batch   850  of  1,028.\n",
      "  Batch   900  of  1,028.\n",
      "  Batch   950  of  1,028.\n",
      "  Batch 1,000  of  1,028.\n",
      "\n",
      "Training Loss: 0.693\n",
      "Validation Loss: 0.692\n",
      "\n",
      " Epoch 4 / 5\n",
      "  Batch    50  of  8,222.\n",
      "  Batch   100  of  8,222.\n",
      "  Batch   150  of  8,222.\n",
      "  Batch   200  of  8,222.\n",
      "  Batch   250  of  8,222.\n",
      "  Batch   300  of  8,222.\n",
      "  Batch   350  of  8,222.\n",
      "  Batch   400  of  8,222.\n",
      "  Batch   450  of  8,222.\n",
      "  Batch   500  of  8,222.\n",
      "  Batch   550  of  8,222.\n",
      "  Batch   600  of  8,222.\n",
      "  Batch   650  of  8,222.\n",
      "  Batch   700  of  8,222.\n",
      "  Batch   750  of  8,222.\n",
      "  Batch   800  of  8,222.\n",
      "  Batch   850  of  8,222.\n",
      "  Batch   900  of  8,222.\n",
      "  Batch   950  of  8,222.\n",
      "  Batch 1,000  of  8,222.\n",
      "  Batch 1,050  of  8,222.\n",
      "  Batch 1,100  of  8,222.\n",
      "  Batch 1,150  of  8,222.\n",
      "  Batch 1,200  of  8,222.\n",
      "  Batch 1,250  of  8,222.\n",
      "  Batch 1,300  of  8,222.\n",
      "  Batch 1,350  of  8,222.\n",
      "  Batch 1,400  of  8,222.\n",
      "  Batch 1,450  of  8,222.\n",
      "  Batch 1,500  of  8,222.\n",
      "  Batch 1,550  of  8,222.\n",
      "  Batch 1,600  of  8,222.\n",
      "  Batch 1,650  of  8,222.\n",
      "  Batch 1,700  of  8,222.\n",
      "  Batch 1,750  of  8,222.\n",
      "  Batch 1,800  of  8,222.\n",
      "  Batch 1,850  of  8,222.\n",
      "  Batch 1,900  of  8,222.\n",
      "  Batch 1,950  of  8,222.\n",
      "  Batch 2,000  of  8,222.\n",
      "  Batch 2,050  of  8,222.\n",
      "  Batch 2,100  of  8,222.\n",
      "  Batch 2,150  of  8,222.\n",
      "  Batch 2,200  of  8,222.\n",
      "  Batch 2,250  of  8,222.\n",
      "  Batch 2,300  of  8,222.\n",
      "  Batch 2,350  of  8,222.\n",
      "  Batch 2,400  of  8,222.\n",
      "  Batch 2,450  of  8,222.\n",
      "  Batch 2,500  of  8,222.\n",
      "  Batch 2,550  of  8,222.\n",
      "  Batch 2,600  of  8,222.\n",
      "  Batch 2,650  of  8,222.\n",
      "  Batch 2,700  of  8,222.\n",
      "  Batch 2,750  of  8,222.\n",
      "  Batch 2,800  of  8,222.\n",
      "  Batch 2,850  of  8,222.\n",
      "  Batch 2,900  of  8,222.\n",
      "  Batch 2,950  of  8,222.\n",
      "  Batch 3,000  of  8,222.\n",
      "  Batch 3,050  of  8,222.\n",
      "  Batch 3,100  of  8,222.\n",
      "  Batch 3,150  of  8,222.\n",
      "  Batch 3,200  of  8,222.\n",
      "  Batch 3,250  of  8,222.\n",
      "  Batch 3,300  of  8,222.\n",
      "  Batch 3,350  of  8,222.\n",
      "  Batch 3,400  of  8,222.\n",
      "  Batch 3,450  of  8,222.\n",
      "  Batch 3,500  of  8,222.\n",
      "  Batch 3,550  of  8,222.\n",
      "  Batch 3,600  of  8,222.\n",
      "  Batch 3,650  of  8,222.\n",
      "  Batch 3,700  of  8,222.\n",
      "  Batch 3,750  of  8,222.\n",
      "  Batch 3,800  of  8,222.\n",
      "  Batch 3,850  of  8,222.\n",
      "  Batch 3,900  of  8,222.\n",
      "  Batch 3,950  of  8,222.\n",
      "  Batch 4,000  of  8,222.\n",
      "  Batch 4,050  of  8,222.\n",
      "  Batch 4,100  of  8,222.\n",
      "  Batch 4,150  of  8,222.\n",
      "  Batch 4,200  of  8,222.\n",
      "  Batch 4,250  of  8,222.\n",
      "  Batch 4,300  of  8,222.\n",
      "  Batch 4,350  of  8,222.\n",
      "  Batch 4,400  of  8,222.\n",
      "  Batch 4,450  of  8,222.\n",
      "  Batch 4,500  of  8,222.\n",
      "  Batch 4,550  of  8,222.\n",
      "  Batch 4,600  of  8,222.\n",
      "  Batch 4,650  of  8,222.\n",
      "  Batch 4,700  of  8,222.\n",
      "  Batch 4,750  of  8,222.\n",
      "  Batch 4,800  of  8,222.\n",
      "  Batch 4,850  of  8,222.\n",
      "  Batch 4,900  of  8,222.\n",
      "  Batch 4,950  of  8,222.\n",
      "  Batch 5,000  of  8,222.\n",
      "  Batch 5,050  of  8,222.\n",
      "  Batch 5,100  of  8,222.\n",
      "  Batch 5,150  of  8,222.\n",
      "  Batch 5,200  of  8,222.\n",
      "  Batch 5,250  of  8,222.\n",
      "  Batch 5,300  of  8,222.\n",
      "  Batch 5,350  of  8,222.\n",
      "  Batch 5,400  of  8,222.\n",
      "  Batch 5,450  of  8,222.\n",
      "  Batch 5,500  of  8,222.\n",
      "  Batch 5,550  of  8,222.\n",
      "  Batch 5,600  of  8,222.\n",
      "  Batch 5,650  of  8,222.\n",
      "  Batch 5,700  of  8,222.\n",
      "  Batch 5,750  of  8,222.\n",
      "  Batch 5,800  of  8,222.\n",
      "  Batch 5,850  of  8,222.\n",
      "  Batch 5,900  of  8,222.\n",
      "  Batch 5,950  of  8,222.\n",
      "  Batch 6,000  of  8,222.\n",
      "  Batch 6,050  of  8,222.\n",
      "  Batch 6,100  of  8,222.\n",
      "  Batch 6,150  of  8,222.\n",
      "  Batch 6,200  of  8,222.\n",
      "  Batch 6,250  of  8,222.\n",
      "  Batch 6,300  of  8,222.\n",
      "  Batch 6,350  of  8,222.\n",
      "  Batch 6,400  of  8,222.\n",
      "  Batch 6,450  of  8,222.\n",
      "  Batch 6,500  of  8,222.\n",
      "  Batch 6,550  of  8,222.\n",
      "  Batch 6,600  of  8,222.\n",
      "  Batch 6,650  of  8,222.\n",
      "  Batch 6,700  of  8,222.\n",
      "  Batch 6,750  of  8,222.\n",
      "  Batch 6,800  of  8,222.\n",
      "  Batch 6,850  of  8,222.\n",
      "  Batch 6,900  of  8,222.\n",
      "  Batch 6,950  of  8,222.\n",
      "  Batch 7,000  of  8,222.\n",
      "  Batch 7,050  of  8,222.\n",
      "  Batch 7,100  of  8,222.\n",
      "  Batch 7,150  of  8,222.\n",
      "  Batch 7,200  of  8,222.\n",
      "  Batch 7,250  of  8,222.\n",
      "  Batch 7,300  of  8,222.\n",
      "  Batch 7,350  of  8,222.\n",
      "  Batch 7,400  of  8,222.\n",
      "  Batch 7,450  of  8,222.\n",
      "  Batch 7,500  of  8,222.\n",
      "  Batch 7,550  of  8,222.\n",
      "  Batch 7,600  of  8,222.\n",
      "  Batch 7,650  of  8,222.\n",
      "  Batch 7,700  of  8,222.\n",
      "  Batch 7,750  of  8,222.\n",
      "  Batch 7,800  of  8,222.\n",
      "  Batch 7,850  of  8,222.\n",
      "  Batch 7,900  of  8,222.\n",
      "  Batch 7,950  of  8,222.\n",
      "  Batch 8,000  of  8,222.\n",
      "  Batch 8,050  of  8,222.\n",
      "  Batch 8,100  of  8,222.\n",
      "  Batch 8,150  of  8,222.\n",
      "  Batch 8,200  of  8,222.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of  1,028.\n",
      "  Batch   100  of  1,028.\n",
      "  Batch   150  of  1,028.\n",
      "  Batch   200  of  1,028.\n",
      "  Batch   250  of  1,028.\n",
      "  Batch   300  of  1,028.\n",
      "  Batch   350  of  1,028.\n",
      "  Batch   400  of  1,028.\n",
      "  Batch   450  of  1,028.\n",
      "  Batch   500  of  1,028.\n",
      "  Batch   550  of  1,028.\n",
      "  Batch   600  of  1,028.\n",
      "  Batch   650  of  1,028.\n",
      "  Batch   700  of  1,028.\n",
      "  Batch   750  of  1,028.\n",
      "  Batch   800  of  1,028.\n",
      "  Batch   850  of  1,028.\n",
      "  Batch   900  of  1,028.\n",
      "  Batch   950  of  1,028.\n",
      "  Batch 1,000  of  1,028.\n",
      "\n",
      "Training Loss: 0.693\n",
      "Validation Loss: 0.692\n",
      "\n",
      " Epoch 5 / 5\n",
      "  Batch    50  of  8,222.\n",
      "  Batch   100  of  8,222.\n",
      "  Batch   150  of  8,222.\n",
      "  Batch   200  of  8,222.\n",
      "  Batch   250  of  8,222.\n",
      "  Batch   300  of  8,222.\n",
      "  Batch   350  of  8,222.\n",
      "  Batch   400  of  8,222.\n",
      "  Batch   450  of  8,222.\n",
      "  Batch   500  of  8,222.\n",
      "  Batch   550  of  8,222.\n",
      "  Batch   600  of  8,222.\n",
      "  Batch   650  of  8,222.\n",
      "  Batch   700  of  8,222.\n",
      "  Batch   750  of  8,222.\n",
      "  Batch   800  of  8,222.\n",
      "  Batch   850  of  8,222.\n",
      "  Batch   900  of  8,222.\n",
      "  Batch   950  of  8,222.\n",
      "  Batch 1,000  of  8,222.\n",
      "  Batch 1,050  of  8,222.\n",
      "  Batch 1,100  of  8,222.\n",
      "  Batch 1,150  of  8,222.\n",
      "  Batch 1,200  of  8,222.\n",
      "  Batch 1,250  of  8,222.\n",
      "  Batch 1,300  of  8,222.\n",
      "  Batch 1,350  of  8,222.\n",
      "  Batch 1,400  of  8,222.\n",
      "  Batch 1,450  of  8,222.\n",
      "  Batch 1,500  of  8,222.\n",
      "  Batch 1,550  of  8,222.\n",
      "  Batch 1,600  of  8,222.\n",
      "  Batch 1,650  of  8,222.\n",
      "  Batch 1,700  of  8,222.\n",
      "  Batch 1,750  of  8,222.\n",
      "  Batch 1,800  of  8,222.\n",
      "  Batch 1,850  of  8,222.\n",
      "  Batch 1,900  of  8,222.\n",
      "  Batch 1,950  of  8,222.\n",
      "  Batch 2,000  of  8,222.\n",
      "  Batch 2,050  of  8,222.\n",
      "  Batch 2,100  of  8,222.\n",
      "  Batch 2,150  of  8,222.\n",
      "  Batch 2,200  of  8,222.\n",
      "  Batch 2,250  of  8,222.\n",
      "  Batch 2,300  of  8,222.\n",
      "  Batch 2,350  of  8,222.\n",
      "  Batch 2,400  of  8,222.\n",
      "  Batch 2,450  of  8,222.\n",
      "  Batch 2,500  of  8,222.\n",
      "  Batch 2,550  of  8,222.\n",
      "  Batch 2,600  of  8,222.\n",
      "  Batch 2,650  of  8,222.\n",
      "  Batch 2,700  of  8,222.\n",
      "  Batch 2,750  of  8,222.\n",
      "  Batch 2,800  of  8,222.\n",
      "  Batch 2,850  of  8,222.\n",
      "  Batch 2,900  of  8,222.\n",
      "  Batch 2,950  of  8,222.\n",
      "  Batch 3,000  of  8,222.\n",
      "  Batch 3,050  of  8,222.\n",
      "  Batch 3,100  of  8,222.\n",
      "  Batch 3,150  of  8,222.\n",
      "  Batch 3,200  of  8,222.\n",
      "  Batch 3,250  of  8,222.\n",
      "  Batch 3,300  of  8,222.\n",
      "  Batch 3,350  of  8,222.\n",
      "  Batch 3,400  of  8,222.\n",
      "  Batch 3,450  of  8,222.\n",
      "  Batch 3,500  of  8,222.\n",
      "  Batch 3,550  of  8,222.\n",
      "  Batch 3,600  of  8,222.\n",
      "  Batch 3,650  of  8,222.\n",
      "  Batch 3,700  of  8,222.\n",
      "  Batch 3,750  of  8,222.\n",
      "  Batch 3,800  of  8,222.\n",
      "  Batch 3,850  of  8,222.\n",
      "  Batch 3,900  of  8,222.\n",
      "  Batch 3,950  of  8,222.\n",
      "  Batch 4,000  of  8,222.\n",
      "  Batch 4,050  of  8,222.\n",
      "  Batch 4,100  of  8,222.\n",
      "  Batch 4,150  of  8,222.\n",
      "  Batch 4,200  of  8,222.\n",
      "  Batch 4,250  of  8,222.\n",
      "  Batch 4,300  of  8,222.\n",
      "  Batch 4,350  of  8,222.\n",
      "  Batch 4,400  of  8,222.\n",
      "  Batch 4,450  of  8,222.\n",
      "  Batch 4,500  of  8,222.\n",
      "  Batch 4,550  of  8,222.\n",
      "  Batch 4,600  of  8,222.\n",
      "  Batch 4,650  of  8,222.\n",
      "  Batch 4,700  of  8,222.\n",
      "  Batch 4,750  of  8,222.\n",
      "  Batch 4,800  of  8,222.\n",
      "  Batch 4,850  of  8,222.\n",
      "  Batch 4,900  of  8,222.\n",
      "  Batch 4,950  of  8,222.\n",
      "  Batch 5,000  of  8,222.\n",
      "  Batch 5,050  of  8,222.\n",
      "  Batch 5,100  of  8,222.\n",
      "  Batch 5,150  of  8,222.\n",
      "  Batch 5,200  of  8,222.\n",
      "  Batch 5,250  of  8,222.\n",
      "  Batch 5,300  of  8,222.\n",
      "  Batch 5,350  of  8,222.\n",
      "  Batch 5,400  of  8,222.\n",
      "  Batch 5,450  of  8,222.\n",
      "  Batch 5,500  of  8,222.\n",
      "  Batch 5,550  of  8,222.\n",
      "  Batch 5,600  of  8,222.\n",
      "  Batch 5,650  of  8,222.\n",
      "  Batch 5,700  of  8,222.\n",
      "  Batch 5,750  of  8,222.\n",
      "  Batch 5,800  of  8,222.\n",
      "  Batch 5,850  of  8,222.\n",
      "  Batch 5,900  of  8,222.\n",
      "  Batch 5,950  of  8,222.\n",
      "  Batch 6,000  of  8,222.\n",
      "  Batch 6,050  of  8,222.\n",
      "  Batch 6,100  of  8,222.\n",
      "  Batch 6,150  of  8,222.\n",
      "  Batch 6,200  of  8,222.\n",
      "  Batch 6,250  of  8,222.\n",
      "  Batch 6,300  of  8,222.\n",
      "  Batch 6,350  of  8,222.\n",
      "  Batch 6,400  of  8,222.\n",
      "  Batch 6,450  of  8,222.\n",
      "  Batch 6,500  of  8,222.\n",
      "  Batch 6,550  of  8,222.\n",
      "  Batch 6,600  of  8,222.\n",
      "  Batch 6,650  of  8,222.\n",
      "  Batch 6,700  of  8,222.\n",
      "  Batch 6,750  of  8,222.\n",
      "  Batch 6,800  of  8,222.\n",
      "  Batch 6,850  of  8,222.\n",
      "  Batch 6,900  of  8,222.\n",
      "  Batch 6,950  of  8,222.\n",
      "  Batch 7,000  of  8,222.\n",
      "  Batch 7,050  of  8,222.\n",
      "  Batch 7,100  of  8,222.\n",
      "  Batch 7,150  of  8,222.\n",
      "  Batch 7,200  of  8,222.\n",
      "  Batch 7,250  of  8,222.\n",
      "  Batch 7,300  of  8,222.\n",
      "  Batch 7,350  of  8,222.\n",
      "  Batch 7,400  of  8,222.\n",
      "  Batch 7,450  of  8,222.\n",
      "  Batch 7,500  of  8,222.\n",
      "  Batch 7,550  of  8,222.\n",
      "  Batch 7,600  of  8,222.\n",
      "  Batch 7,650  of  8,222.\n",
      "  Batch 7,700  of  8,222.\n",
      "  Batch 7,750  of  8,222.\n",
      "  Batch 7,800  of  8,222.\n",
      "  Batch 7,850  of  8,222.\n",
      "  Batch 7,900  of  8,222.\n",
      "  Batch 7,950  of  8,222.\n",
      "  Batch 8,000  of  8,222.\n",
      "  Batch 8,050  of  8,222.\n",
      "  Batch 8,100  of  8,222.\n",
      "  Batch 8,150  of  8,222.\n",
      "  Batch 8,200  of  8,222.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of  1,028.\n",
      "  Batch   100  of  1,028.\n",
      "  Batch   150  of  1,028.\n",
      "  Batch   200  of  1,028.\n",
      "  Batch   250  of  1,028.\n",
      "  Batch   300  of  1,028.\n",
      "  Batch   350  of  1,028.\n",
      "  Batch   400  of  1,028.\n",
      "  Batch   450  of  1,028.\n",
      "  Batch   500  of  1,028.\n",
      "  Batch   550  of  1,028.\n",
      "  Batch   600  of  1,028.\n",
      "  Batch   650  of  1,028.\n",
      "  Batch   700  of  1,028.\n",
      "  Batch   750  of  1,028.\n",
      "  Batch   800  of  1,028.\n",
      "  Batch   850  of  1,028.\n",
      "  Batch   900  of  1,028.\n",
      "  Batch   950  of  1,028.\n",
      "  Batch 1,000  of  1,028.\n",
      "\n",
      "Training Loss: 0.693\n",
      "Validation Loss: 0.693\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "#defining epochs\n",
    "epochs = 5\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate(val_dataloader)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 9.41 GiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Of the allocated memory 14.67 GiB is allocated by PyTorch, and 9.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# get predictions for test data \u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \n\u001b[1;32m----> 5\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      6\u001b[0m     preds \u001b[38;5;241m=\u001b[39m preds\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# model's performance\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[68], line 27\u001b[0m, in \u001b[0;36mBERT_Arch.forward\u001b[1;34m(self, sent_id, mask)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sent_id, mask):\n\u001b[0;32m     25\u001b[0m     \n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m#pass the inputs to the model  \u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     _, cls_hs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(cls_hs)\n\u001b[0;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1012\u001b[0m )\n\u001b[1;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    598\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         output_attentions,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    536\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    537\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 539\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    544\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pytorch_utils.py:237\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 551\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:452\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    451\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m--> 452\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\activations.py:79\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 9.41 GiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Of the allocated memory 14.67 GiB is allocated by PyTorch, and 9.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# get predictions for test data \n",
    "with torch.no_grad(): \n",
    "    preds = model(test_seq.to(device), test_mask.to(device)) \n",
    "    preds = preds.detach().cpu().numpy() \n",
    " \n",
    "\n",
    "# model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT_Arch(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch \n",
    " \n",
    "# # save tagged_data \n",
    "# # with open('tagged_data.pickle', 'wb') as handle: \n",
    "# #     pickle.dump(tagged_data, handle, protocol=pickle.HIGHEST_PROTOCOL) \n",
    " \n",
    "# # open and load tags from pickle file \n",
    "# model = BERT_Arch(bert)\n",
    "# # model = BERT_Arch(*args, **kwargs)\n",
    "# model.load_state_dict(torch.load('saved_weights2.pt'))\n",
    "# model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n",
      "  Batch    50  of  1,028.\n",
      "  Batch   100  of  1,028.\n",
      "  Batch   150  of  1,028.\n",
      "  Batch   200  of  1,028.\n",
      "  Batch   250  of  1,028.\n",
      "  Batch   300  of  1,028.\n",
      "  Batch   350  of  1,028.\n",
      "  Batch   400  of  1,028.\n",
      "  Batch   450  of  1,028.\n",
      "  Batch   500  of  1,028.\n",
      "  Batch   550  of  1,028.\n",
      "  Batch   600  of  1,028.\n",
      "  Batch   650  of  1,028.\n",
      "  Batch   700  of  1,028.\n",
      "  Batch   750  of  1,028.\n",
      "  Batch   800  of  1,028.\n",
      "  Batch   850  of  1,028.\n",
      "  Batch   900  of  1,028.\n",
      "  Batch   950  of  1,028.\n",
      "  Batch 1,000  of  1,028.\n"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "loss, tt_preds = evaluate(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      1.00      0.68     16809\n",
      "           1       0.00      0.00      0.00     16078\n",
      "\n",
      "    accuracy                           0.51     32887\n",
      "   macro avg       0.26      0.50      0.34     32887\n",
      "weighted avg       0.26      0.51      0.35     32887\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CasKei\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\CasKei\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\CasKei\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# # get predictions for test data \n",
    "# with torch.no_grad(): \n",
    "#     preds = model(test_seq.to(device), test_mask.to(device)) \n",
    "#     preds = preds.detach().cpu().numpy() \n",
    " \n",
    " \n",
    "# # model's performance \n",
    "# preds = np.argmax(preds, axis = 1) \n",
    "# print(classification_report(test_y, preds))\n",
    "\n",
    "\n",
    "# tt_preds = tt_preds.to('cpu')\n",
    "\n",
    "torch_tensor = torch.from_numpy(tt_preds)\n",
    "cpu_tensor = torch_tensor.cpu()\n",
    "\n",
    "# model's performance \n",
    "preds = np.argmax(cpu_tensor, axis = 1) \n",
    "print(classification_report(test_y.cpu(), preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.073747   -0.13434753]\n",
      " [-0.7725483  -0.61958945]\n",
      " [-0.29789892 -1.3562554 ]\n",
      " ...\n",
      " [-0.12222891 -2.1623516 ]\n",
      " [-0.03088112 -3.4930108 ]\n",
      " [-0.35752735 -1.2019867 ]]\n"
     ]
    }
   ],
   "source": [
    "print(tt_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1, device='cuda:0')\n",
      "[-0.54296535 -0.86993873]\n"
     ]
    }
   ],
   "source": [
    "print(test_y[22])\n",
    "print(tt_preds[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
