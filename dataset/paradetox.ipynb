{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using the pre-trained available BART-ParaDetox model, we run all of the comments in Jigsaw dataset through it to generate its parallel detoxed dataset.\n",
    "2. Using the generated parallel dataset, we use it to train our own BART model\n",
    "3. The trained BART model will then be used to generate the detoxified sentence of any input sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the python environment\n",
    "Ideally you run the following cells with the virtual environment created using python3 (anything before 3.10 version works from our testing) with the packages specified in the `requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and load the Jigsaw dataset with the relevant columns:\n",
    "- `comment_text` : the column containing the input toxic sentence\n",
    "- `toxicity` : the column cotaining the toxicity score of the input toxic sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating the parallel dataset from Jigsaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('all_data.csv')\n",
    "target_columns = ['comment_text', 'toxicity', 'severe_toxicity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r'http\\S+', '', text) # Remove URLs\n",
    "        text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "        text = text.lower()\n",
    "        # Handle self-censored words and emojis as needed\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        return \"\"\n",
    "    return text\n",
    "\n",
    "df['cleaned_text']= df['comment_text'].apply(clean_text)\n",
    "# cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detoxifying texts: 100%|██████████| 4/4 [1:35:37<00:00, 1434.31s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the pretrained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"s-nlp/bart-base-detox\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"s-nlp/bart-base-detox\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)  # Move the model to the desired device (CPU or GPU)\n",
    "\n",
    "# Create lists to store the dataframes\n",
    "jigsaw_parallel_data = []\n",
    "jigsaw_strict_parallel_data = []\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Iterate over the rows in the dataframe in batches\n",
    "# len(df) for whole dataset, eg 200 for first 200 rows\n",
    "# Note that if num_rows_to_process is not divisible by batch_size, the last batch will contain fewer rows than batch_size.\n",
    "for start_idx in tqdm(range(0, 200, batch_size), desc=\"Detoxifying texts\"):\n",
    "    end_idx = start_idx + batch_size\n",
    "    batch_texts = df['cleaned_text'][start_idx:end_idx].tolist()\n",
    "    batch_toxicity = df['toxicity'][start_idx:end_idx].tolist()\n",
    "    batch_severe_toxicity = df['severe_toxicity'][start_idx:end_idx].tolist()\n",
    "\n",
    "    # Tokenize the input texts\n",
    "    tokens = tokenizer(batch_texts, return_tensors='pt', padding=True)\n",
    "    tokens = tokens.to(device)  # Move the tensors to the desired device\n",
    "\n",
    "    # Generate the detoxified texts\n",
    "    output_tokens = model.generate(**tokens, num_return_sequences=1, do_sample=False,\n",
    "                                   temperature=1.0, repetition_penalty=10.0,\n",
    "                                   max_length=128, num_beams=10)\n",
    "\n",
    "    # Decode the output tokens to get the detoxified texts\n",
    "    detoxed_texts = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # Append the data to the respective lists\n",
    "    for text, detoxed_text, toxicity, severe_toxicity in zip(batch_texts, detoxed_texts, batch_toxicity, batch_severe_toxicity):\n",
    "        jigsaw_parallel_data.append({'pre-detoxed': text, 'detoxed_text': detoxed_text, 'toxicity': toxicity, 'severe_toxicity': severe_toxicity})\n",
    "        if text != detoxed_text:\n",
    "            jigsaw_strict_parallel_data.append({'pre-detoxed': text, 'detoxed_text': detoxed_text, 'toxicity': toxicity, 'severe_toxicity': severe_toxicity})\n",
    "\n",
    "# Create dataframes from the lists\n",
    "jigsaw_parallel = pd.DataFrame(jigsaw_parallel_data)\n",
    "jigsaw_strict_parallel = pd.DataFrame(jigsaw_strict_parallel_data)\n",
    "\n",
    "# Save the dataframes to CSV files\n",
    "jigsaw_parallel.to_csv('jigsaw_parallel.csv', index=False)\n",
    "jigsaw_strict_parallel.to_csv('jigsaw_strict_parallel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cant fix bad but we are electing them to public office due in large part to the silent majority who choose not to vote\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"s-nlp/bart-base-detox\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"s-nlp/bart-base-detox\")\n",
    "# 'that sick fuck is going to be out in 54 years.', \n",
    "toxics = [\"cant fix stupid but we are electing them to public office due in large part to the silent majority who choose not to vote\"]\n",
    "tokens = tokenizer(toxics, return_tensors='pt', padding=True)\n",
    "tokens = model.generate(**tokens, num_return_sequences=5, do_sample=False,\n",
    "                        temperature=1.0, repetition_penalty=10.0,\n",
    "                        max_length=128, num_beams=10)\n",
    "neutrals = tokenizer.decode(tokens[0, ...], skip_special_tokens=True)\n",
    "print(neutrals) # stdout: She is going to be out in 54 years.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
