{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using the pre-trained available BART-ParaDetox model, we run all of the comments in Jigsaw dataset through it to generate its parallel detoxed dataset.\n",
    "2. Using the generated parallel dataset, we use it to train our own BART model\n",
    "3. The trained BART model will then be used to generate the detoxified sentence of any input sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the python environment\n",
    "Ideally you run the following cells with the virtual environment created using python3 (anything before 3.10 version works from our testing) with the packages specified in the `requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# for cleaning later\n",
    "from transformers import BertTokenizer\n",
    "import spacy\n",
    "# from emoji import demojize\n",
    "import string\n",
    "# spacy.cli.download(\"en_core_web_sm\")\n",
    "# nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and load the Jigsaw dataset with the relevant columns:\n",
    "- `comment_text` : the column containing the input toxic sentence\n",
    "- `toxicity` : the column cotaining the toxicity score of the input toxic sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating the parallel dataset from Jigsaw\n",
    "We use `cleaned_balanced_data.csv` as it is a smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('all_data.csv')\n",
    "target_columns = ['comment_text', 'toxicity', 'severe_toxicity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Convert emojis to text descriptions\n",
    "        # text = demojize(text)\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Remove stopwords\n",
    "        # stop_words = set(stopwords.words('english'))\n",
    "        # text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "        \n",
    "        # Handle contractions\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "        text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'s\", \" is\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'m\", \" am\", text)\n",
    "        \n",
    "        # Handle self-censored text\n",
    "        text = re.sub(r'\\*+', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    else:\n",
    "        text = ''\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "df['cleaned_text']= df['comment_text'].apply(clean_text)\n",
    "df.to_csv('clean_data.csv', index=False)\n",
    "# cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detoxifying texts: 100%|██████████| 32/32 [1:25:40<00:00, 160.64s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the pretrained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"s-nlp/bart-base-detox\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"s-nlp/bart-base-detox\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)  # Move the model to the desired device (CPU or GPU)\n",
    "\n",
    "# Create lists to store the dataframes\n",
    "jigsaw_parallel_data = []\n",
    "jigsaw_strict_parallel_data = []\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Iterate over the rows in the dataframe in batches\n",
    "# len(df) for whole dataset, eg 200 for first 200 rows\n",
    "# Note that if num_rows_to_process is not divisible by batch_size, the last batch will contain fewer rows than batch_size.\n",
    "for start_idx in tqdm(range(0, 1000, batch_size), desc=\"Detoxifying texts\"):\n",
    "    end_idx = start_idx + batch_size\n",
    "    batch_texts = df['cleaned_text'][start_idx:end_idx].tolist()\n",
    "    batch_toxicity = df['toxicity'][start_idx:end_idx].tolist()\n",
    "    batch_severe_toxicity = df['severe_toxicity'][start_idx:end_idx].tolist()\n",
    "\n",
    "    # Tokenize the input texts\n",
    "    tokens = tokenizer(batch_texts, return_tensors='pt', padding=True)\n",
    "    tokens = tokens.to(device)  # Move the tensors to the desired device\n",
    "\n",
    "    # Generate the detoxified texts\n",
    "    output_tokens = model.generate(**tokens, num_return_sequences=1, do_sample=False,\n",
    "                                   temperature=1.0, repetition_penalty=10.0,\n",
    "                                   max_length=128, num_beams=10)\n",
    "\n",
    "    # Decode the output tokens to get the detoxified texts\n",
    "    detoxed_texts = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # Append the data to the respective lists\n",
    "    for text, detoxed_text, toxicity, severe_toxicity in zip(batch_texts, detoxed_texts, batch_toxicity, batch_severe_toxicity):\n",
    "        jigsaw_parallel_data.append({'pre-detoxed': text, 'detoxed_text': detoxed_text, 'toxicity': toxicity, 'severe_toxicity': severe_toxicity})\n",
    "        if text != detoxed_text:\n",
    "            jigsaw_strict_parallel_data.append({'pre-detoxed': text, 'detoxed_text': detoxed_text, 'toxicity': toxicity, 'severe_toxicity': severe_toxicity})\n",
    "\n",
    "# Create dataframes from the lists\n",
    "jigsaw_parallel = pd.DataFrame(jigsaw_parallel_data)\n",
    "jigsaw_strict_parallel = pd.DataFrame(jigsaw_strict_parallel_data)\n",
    "\n",
    "# Save the dataframes to CSV files\n",
    "jigsaw_parallel.to_csv('jigsaw_parallel.csv', index=False)\n",
    "jigsaw_strict_parallel.to_csv('jigsaw_strict_parallel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detoxifying texts: 100%|██████████| 125/125 [5:37:49<00:00, 162.16s/it]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the pretrained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"s-nlp/bart-base-detox\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"s-nlp/bart-base-detox\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)  # Move the model to the desired device (CPU or GPU)\n",
    "\n",
    "# Create lists to store the dataframes\n",
    "jigsaw_parallel_data = []\n",
    "jigsaw_strict_parallel_data = []\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Iterate over the rows in the dataframe in batches\n",
    "# len(df) for whole dataset, eg 200 for first 200 rows\n",
    "# Note that if num_rows_to_process is not divisible by batch_size, the last batch will contain fewer rows than batch_size.\n",
    "for start_idx in tqdm(range(0, 4000, batch_size), desc=\"Detoxifying texts\"):\n",
    "    end_idx = start_idx + batch_size + 1000\n",
    "    batch_texts = df['cleaned_text'][start_idx+1000:end_idx].tolist()\n",
    "    batch_toxicity = df['toxicity'][start_idx+1000:end_idx].tolist()\n",
    "    batch_severe_toxicity = df['severe_toxicity'][start_idx+1000:end_idx].tolist()\n",
    "\n",
    "    # Tokenize the input texts\n",
    "    tokens = tokenizer(batch_texts, return_tensors='pt', padding=True)\n",
    "    tokens = tokens.to(device)  # Move the tensors to the desired device\n",
    "\n",
    "    # Generate the detoxified texts\n",
    "    output_tokens = model.generate(**tokens, num_return_sequences=1, do_sample=False,\n",
    "                                   temperature=1.0, repetition_penalty=10.0,\n",
    "                                   max_length=128, num_beams=10)\n",
    "\n",
    "    # Decode the output tokens to get the detoxified texts\n",
    "    detoxed_texts = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # Append the data to the respective lists\n",
    "    for text, detoxed_text, toxicity, severe_toxicity in zip(batch_texts, detoxed_texts, batch_toxicity, batch_severe_toxicity):\n",
    "        jigsaw_parallel_data.append({'pre-detoxed': text, 'detoxed_text': detoxed_text, 'toxicity': toxicity, 'severe_toxicity': severe_toxicity})\n",
    "        if text != detoxed_text:\n",
    "            jigsaw_strict_parallel_data.append({'pre-detoxed': text, 'detoxed_text': detoxed_text, 'toxicity': toxicity, 'severe_toxicity': severe_toxicity})\n",
    "\n",
    "# Create dataframes from the lists\n",
    "jigsaw_parallel = pd.DataFrame(jigsaw_parallel_data)\n",
    "jigsaw_strict_parallel = pd.DataFrame(jigsaw_strict_parallel_data)\n",
    "\n",
    "# Save the dataframes to CSV files\n",
    "jigsaw_parallel.to_csv('jigsaw_parallel2.csv', index=False)\n",
    "jigsaw_strict_parallel.to_csv('jigsaw_strict_parallel2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please see `train_bart.ipynb` in the paradetox-main folder for subsequent training and evaluation of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
