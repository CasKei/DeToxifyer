{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'split', 'created_date', 'publication_id',\n",
       "       'parent_id', 'article_id', 'rating', 'funny', 'wow', 'sad', 'likes',\n",
       "       'disagree', 'toxicity', 'severe_toxicity', 'obscene', 'sexual_explicit',\n",
       "       'identity_attack', 'insult', 'threat', 'male', 'female', 'transgender',\n",
       "       'other_gender', 'heterosexual', 'homosexual_gay_or_lesbian', 'bisexual',\n",
       "       'other_sexual_orientation', 'christian', 'jewish', 'muslim', 'hindu',\n",
       "       'buddhist', 'atheist', 'other_religion', 'black', 'white', 'asian',\n",
       "       'latino', 'other_race_or_ethnicity', 'physical_disability',\n",
       "       'intellectual_or_learning_disability', 'psychiatric_or_mental_illness',\n",
       "       'other_disability', 'identity_annotator_count',\n",
       "       'toxicity_annotator_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('all_data.csv')\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>split</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>596097</th>\n",
       "      <td>5801652</td>\n",
       "      <td>Awesome! Lets cut the head off hate! Lets stab...</td>\n",
       "      <td>train</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>368010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207189</th>\n",
       "      <td>5997855</td>\n",
       "      <td>You are murdering feckless SCUM.  Your mother ...</td>\n",
       "      <td>train</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>379957</td>\n",
       "      <td>0.983501</td>\n",
       "      <td>0.644363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601973</th>\n",
       "      <td>5883664</td>\n",
       "      <td>.\\n.\\nIs there really a God ?\\n... I once thou...</td>\n",
       "      <td>train</td>\n",
       "      <td>53</td>\n",
       "      <td>5879007.0</td>\n",
       "      <td>373036</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461912</th>\n",
       "      <td>6180251</td>\n",
       "      <td>DOGS----\\nEAT\\nSHIT\\nSLEEP\\nBARK\\n\\nNIGERS\\nEA...</td>\n",
       "      <td>train</td>\n",
       "      <td>21</td>\n",
       "      <td>6179746.0</td>\n",
       "      <td>390746</td>\n",
       "      <td>0.990396</td>\n",
       "      <td>0.591236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528865</th>\n",
       "      <td>5754942</td>\n",
       "      <td>BURN THIS DOG RAPING WHITE NIGER ALIVE\\nTIME f...</td>\n",
       "      <td>train</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365110</td>\n",
       "      <td>0.973936</td>\n",
       "      <td>0.569070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709894</th>\n",
       "      <td>5163549</td>\n",
       "      <td>Burnsie: I wish you all the best, but despite ...</td>\n",
       "      <td>train</td>\n",
       "      <td>54</td>\n",
       "      <td>5163479.0</td>\n",
       "      <td>328802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709893</th>\n",
       "      <td>5687703</td>\n",
       "      <td>Yup.</td>\n",
       "      <td>train</td>\n",
       "      <td>43</td>\n",
       "      <td>5687001.0</td>\n",
       "      <td>360936</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709892</th>\n",
       "      <td>5121964</td>\n",
       "      <td>ANOTHER moped rider speeding AND not wearing a...</td>\n",
       "      <td>train</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>326223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709891</th>\n",
       "      <td>5317323</td>\n",
       "      <td>Scheer was born in Ottawa, Ontario, 1979\\n-stu...</td>\n",
       "      <td>train</td>\n",
       "      <td>54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>338336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999515</th>\n",
       "      <td>4984105</td>\n",
       "      <td>You know the Trump fanatics are trolling the G...</td>\n",
       "      <td>train</td>\n",
       "      <td>54</td>\n",
       "      <td>807615.0</td>\n",
       "      <td>156960</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1999516 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                       comment_text  split  \\\n",
       "596097   5801652  Awesome! Lets cut the head off hate! Lets stab...  train   \n",
       "207189   5997855  You are murdering feckless SCUM.  Your mother ...  train   \n",
       "601973   5883664  .\\n.\\nIs there really a God ?\\n... I once thou...  train   \n",
       "461912   6180251  DOGS----\\nEAT\\nSHIT\\nSLEEP\\nBARK\\n\\nNIGERS\\nEA...  train   \n",
       "528865   5754942  BURN THIS DOG RAPING WHITE NIGER ALIVE\\nTIME f...  train   \n",
       "...          ...                                                ...    ...   \n",
       "709894   5163549  Burnsie: I wish you all the best, but despite ...  train   \n",
       "709893   5687703                                               Yup.  train   \n",
       "709892   5121964  ANOTHER moped rider speeding AND not wearing a...  train   \n",
       "709891   5317323  Scheer was born in Ottawa, Ontario, 1979\\n-stu...  train   \n",
       "1999515  4984105  You know the Trump fanatics are trolling the G...  train   \n",
       "\n",
       "         publication_id  parent_id  article_id  toxicity  severe_toxicity  \n",
       "596097               21        NaN      368010  1.000000         1.000000  \n",
       "207189              102        NaN      379957  0.983501         0.644363  \n",
       "601973               53  5879007.0      373036  0.900000         0.600000  \n",
       "461912               21  6179746.0      390746  0.990396         0.591236  \n",
       "528865              105        NaN      365110  0.973936         0.569070  \n",
       "...                 ...        ...         ...       ...              ...  \n",
       "709894               54  5163479.0      328802  0.000000         0.000000  \n",
       "709893               43  5687001.0      360936  0.000000         0.000000  \n",
       "709892               55        NaN      326223  0.000000         0.000000  \n",
       "709891               54        NaN      338336  0.000000         0.000000  \n",
       "1999515              54   807615.0      156960  0.400000         0.000000  \n",
       "\n",
       "[1999516 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf = df[['id', 'comment_text', 'split', 'publication_id',\n",
    "       'parent_id', 'article_id', 'toxicity', 'severe_toxicity']]\n",
    "\n",
    "newdf = newdf.sort_values(by='severe_toxicity', ascending=False)\n",
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          he got his money now he lies in wait till afte...\n",
       "1          mad dog will surely put the liberals in mental...\n",
       "2          and trump continues his lifelong cowardice by ...\n",
       "3          while arresting a man for resisting arrest\\n\\n...\n",
       "4               tucker and paul are both total bad ass mofos\n",
       "                                 ...                        \n",
       "1999511    another man shamming article if white men did ...\n",
       "1999512    no matter what is put in front of you regardin...\n",
       "1999513    the democrat party aided and abetted by its ms...\n",
       "1999514    i just dont find her a very good representatio...\n",
       "1999515    you know the trump fanatics are trolling the g...\n",
       "Name: comment_text, Length: 1999516, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r'http\\S+', '', text) # Remove URLs\n",
    "        text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "        text = text.lower()\n",
    "        # Handle self-censored words and emojis as needed\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        return \"\"\n",
    "    return text\n",
    "\n",
    "cleaned = df['comment_text'].apply(clean_text)\n",
    "cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec,\\\n",
    "    TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess to make a big list of sentences\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d), tags=[str(i)]) for i, _d in enumerate(cleaned)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save tagged_data\n",
    "with open('tagged_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(tagged_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the Doc2vec model\n",
    "model = Doc2Vec(vector_size=20,\n",
    "                min_count=2, epochs=10)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data,\n",
    "            total_examples=model.corpus_count,\n",
    "            epochs=model.epochs)\n",
    " \n",
    "# get the document vectors\n",
    "document_vectors = [model.infer_vector(\n",
    "    word_tokenize(doc)) for doc in cleaned]\n",
    " \n",
    "#  print the document vectors\n",
    "for i, doc in enumerate(cleaned):\n",
    "    print(\"Document\", i+1, \":\", doc)\n",
    "    print(\"Vector:\", document_vectors[i])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
