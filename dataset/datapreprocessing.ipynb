{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Processing for Logistic Regression"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "import numpy as np\n",
            "import pandas as pd\n",
            "import seaborn as sns\n",
            "import matplotlib.pyplot as plt\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "Index(['id', 'comment_text', 'split', 'created_date', 'publication_id',\n",
                     "       'parent_id', 'article_id', 'rating', 'funny', 'wow', 'sad', 'likes',\n",
                     "       'disagree', 'toxicity', 'severe_toxicity', 'obscene', 'sexual_explicit',\n",
                     "       'identity_attack', 'insult', 'threat', 'male', 'female', 'transgender',\n",
                     "       'other_gender', 'heterosexual', 'homosexual_gay_or_lesbian', 'bisexual',\n",
                     "       'other_sexual_orientation', 'christian', 'jewish', 'muslim', 'hindu',\n",
                     "       'buddhist', 'atheist', 'other_religion', 'black', 'white', 'asian',\n",
                     "       'latino', 'other_race_or_ethnicity', 'physical_disability',\n",
                     "       'intellectual_or_learning_disability', 'psychiatric_or_mental_illness',\n",
                     "       'other_disability', 'identity_annotator_count',\n",
                     "       'toxicity_annotator_count'],\n",
                     "      dtype='object')"
                  ]
               },
               "execution_count": 6,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "df = pd.read_csv('all_data.csv')\n",
            "\n",
            "df.columns"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## A bit useless"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>id</th>\n",
                     "      <th>comment_text</th>\n",
                     "      <th>split</th>\n",
                     "      <th>publication_id</th>\n",
                     "      <th>parent_id</th>\n",
                     "      <th>article_id</th>\n",
                     "      <th>toxicity</th>\n",
                     "      <th>severe_toxicity</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>596097</th>\n",
                     "      <td>5801652</td>\n",
                     "      <td>Awesome! Lets cut the head off hate! Lets stab...</td>\n",
                     "      <td>train</td>\n",
                     "      <td>21</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>368010</td>\n",
                     "      <td>1.000000</td>\n",
                     "      <td>1.000000</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>207189</th>\n",
                     "      <td>5997855</td>\n",
                     "      <td>You are murdering feckless SCUM.  Your mother ...</td>\n",
                     "      <td>train</td>\n",
                     "      <td>102</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>379957</td>\n",
                     "      <td>0.983501</td>\n",
                     "      <td>0.644363</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>601973</th>\n",
                     "      <td>5883664</td>\n",
                     "      <td>.\\n.\\nIs there really a God ?\\n... I once thou...</td>\n",
                     "      <td>train</td>\n",
                     "      <td>53</td>\n",
                     "      <td>5879007.0</td>\n",
                     "      <td>373036</td>\n",
                     "      <td>0.900000</td>\n",
                     "      <td>0.600000</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>461912</th>\n",
                     "      <td>6180251</td>\n",
                     "      <td>DOGS----\\nEAT\\nSHIT\\nSLEEP\\nBARK\\n\\nNIGERS\\nEA...</td>\n",
                     "      <td>train</td>\n",
                     "      <td>21</td>\n",
                     "      <td>6179746.0</td>\n",
                     "      <td>390746</td>\n",
                     "      <td>0.990396</td>\n",
                     "      <td>0.591236</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>528865</th>\n",
                     "      <td>5754942</td>\n",
                     "      <td>BURN THIS DOG RAPING WHITE NIGER ALIVE\\nTIME f...</td>\n",
                     "      <td>train</td>\n",
                     "      <td>105</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>365110</td>\n",
                     "      <td>0.973936</td>\n",
                     "      <td>0.569070</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>...</th>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>709894</th>\n",
                     "      <td>5163549</td>\n",
                     "      <td>Burnsie: I wish you all the best, but despite ...</td>\n",
                     "      <td>train</td>\n",
                     "      <td>54</td>\n",
                     "      <td>5163479.0</td>\n",
                     "      <td>328802</td>\n",
                     "      <td>0.000000</td>\n",
                     "      <td>0.000000</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>709893</th>\n",
                     "      <td>5687703</td>\n",
                     "      <td>Yup.</td>\n",
                     "      <td>train</td>\n",
                     "      <td>43</td>\n",
                     "      <td>5687001.0</td>\n",
                     "      <td>360936</td>\n",
                     "      <td>0.000000</td>\n",
                     "      <td>0.000000</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>709892</th>\n",
                     "      <td>5121964</td>\n",
                     "      <td>ANOTHER moped rider speeding AND not wearing a...</td>\n",
                     "      <td>train</td>\n",
                     "      <td>55</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>326223</td>\n",
                     "      <td>0.000000</td>\n",
                     "      <td>0.000000</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>709891</th>\n",
                     "      <td>5317323</td>\n",
                     "      <td>Scheer was born in Ottawa, Ontario, 1979\\n-stu...</td>\n",
                     "      <td>train</td>\n",
                     "      <td>54</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>338336</td>\n",
                     "      <td>0.000000</td>\n",
                     "      <td>0.000000</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>1999515</th>\n",
                     "      <td>4984105</td>\n",
                     "      <td>You know the Trump fanatics are trolling the G...</td>\n",
                     "      <td>train</td>\n",
                     "      <td>54</td>\n",
                     "      <td>807615.0</td>\n",
                     "      <td>156960</td>\n",
                     "      <td>0.400000</td>\n",
                     "      <td>0.000000</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "<p>1999516 rows Ã— 8 columns</p>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "              id                                       comment_text  split  \\\n",
                     "596097   5801652  Awesome! Lets cut the head off hate! Lets stab...  train   \n",
                     "207189   5997855  You are murdering feckless SCUM.  Your mother ...  train   \n",
                     "601973   5883664  .\\n.\\nIs there really a God ?\\n... I once thou...  train   \n",
                     "461912   6180251  DOGS----\\nEAT\\nSHIT\\nSLEEP\\nBARK\\n\\nNIGERS\\nEA...  train   \n",
                     "528865   5754942  BURN THIS DOG RAPING WHITE NIGER ALIVE\\nTIME f...  train   \n",
                     "...          ...                                                ...    ...   \n",
                     "709894   5163549  Burnsie: I wish you all the best, but despite ...  train   \n",
                     "709893   5687703                                               Yup.  train   \n",
                     "709892   5121964  ANOTHER moped rider speeding AND not wearing a...  train   \n",
                     "709891   5317323  Scheer was born in Ottawa, Ontario, 1979\\n-stu...  train   \n",
                     "1999515  4984105  You know the Trump fanatics are trolling the G...  train   \n",
                     "\n",
                     "         publication_id  parent_id  article_id  toxicity  severe_toxicity  \n",
                     "596097               21        NaN      368010  1.000000         1.000000  \n",
                     "207189              102        NaN      379957  0.983501         0.644363  \n",
                     "601973               53  5879007.0      373036  0.900000         0.600000  \n",
                     "461912               21  6179746.0      390746  0.990396         0.591236  \n",
                     "528865              105        NaN      365110  0.973936         0.569070  \n",
                     "...                 ...        ...         ...       ...              ...  \n",
                     "709894               54  5163479.0      328802  0.000000         0.000000  \n",
                     "709893               43  5687001.0      360936  0.000000         0.000000  \n",
                     "709892               55        NaN      326223  0.000000         0.000000  \n",
                     "709891               54        NaN      338336  0.000000         0.000000  \n",
                     "1999515              54   807615.0      156960  0.400000         0.000000  \n",
                     "\n",
                     "[1999516 rows x 8 columns]"
                  ]
               },
               "execution_count": 7,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "newdf = df[['id', 'comment_text', 'split', 'publication_id',\n",
            "       'parent_id', 'article_id', 'toxicity', 'severe_toxicity']]\n",
            "\n",
            "newdf = newdf.sort_values(by='severe_toxicity', ascending=False)\n",
            "newdf"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Data Cleaning"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "import re\n",
            "def clean_text(text):\n",
            "    if isinstance(text, str):\n",
            "        text = re.sub(r'http\\S+', '', text) # Remove URLs\n",
            "        text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
            "        text = text.lower()\n",
            "        # Handle self-censored words and emojis as needed\n",
            "        pass\n",
            "    \n",
            "    else:\n",
            "        return \"\"\n",
            "    return text\n",
            "\n",
            "# cleaned = df['comment_text'].apply(clean_text)\n",
            "# cleaned\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Delete rows to 'balance' the dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "is_toxic\n",
                  "0    1570549\n",
                  "1     428967\n",
                  "Name: count, dtype: int64\n"
               ]
            }
         ],
         "source": [
            "# Read data and train test split\n",
            "data = pd.read_csv('all_data.csv')\n",
            "data['cleaned_text'] = data['comment_text'].apply(clean_text)\n",
            "data.columns\n",
            "# Specify the columns to consider for toxicity\n",
            "toxic_columns = ['toxicity', 'severe_toxicity', 'obscene', 'sexual_explicit', 'identity_attack', 'insult', 'threat']\n",
            "\n",
            "# # function to normalise data (?) \n",
            "# def normalise_x(inputX): \n",
            "#     min_max_scaler = preprocessing.MinMaxScaler() \n",
            "#     dfToNorm_scaled = min_max_scaler.fit_transform(inputX) \n",
            "#     x = pd.DataFrame(dfToNorm_scaled) \n",
            "#     return x\n",
            "\n",
            "# Create a MinMaxScaler object\n",
            "# scaler = preprocessing.MinMaxScaler()\n",
            "\n",
            "# Normalize the specified columns \n",
            "#data[toxic_columns] = scaler.fit_transform(data[toxic_columns])\n",
            "\n",
            "# Print the normalized data\n",
            "#print(\"Normalized Data\")\n",
            "#print(data[toxic_columns].head())\n",
            "\n",
            "\n",
            "# Update the 'toxicity' label based on the values in the specified columns\n",
            "data['is_toxic'] = 0\n",
            "data['is_toxic'] = data.apply(lambda row: 1 if any(row[col] >= 0.2 for col in toxic_columns) else row['is_toxic'], axis=1)\n",
            "\n",
            "# Check the updated class distribution\n",
            "print(data['is_toxic'].value_counts())\n",
            "\n",
            "# Calculate the ratio of toxic to non-toxic samples\n",
            "toxic_count = data['is_toxic'].sum()\n",
            "non_toxic_count = len(data) - toxic_count\n",
            "ratio = non_toxic_count / toxic_count\n",
            "\n",
            "# If the ratio is still above a certain threshold, remove some non-toxic samples\n",
            "if ratio > 2:  # Adjust the threshold as per your requirement\n",
            "    non_toxic_data = data[data['is_toxic'] == 0]\n",
            "    toxic_data = data[data['is_toxic'] == 1]\n",
            "    \n",
            "    # Randomly select a subset of non-toxic samples to keep\n",
            "    num_samples_to_keep = int(toxic_count)  # Adjust the factor as per your requirement\n",
            "    non_toxic_data = non_toxic_data.sample(n=num_samples_to_keep, random_state=42)\n",
            "    \n",
            "    # Combine the toxic and selected non-toxic samples\n",
            "    data = pd.concat([toxic_data, non_toxic_data])\n",
            "    \n",
            "    # Shuffle the data\n",
            "    data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Save the processed data as a new csv"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [],
         "source": [
            "data.to_csv('cleaned_data.csv', index=False)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Tokenization"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[nltk_data] Downloading package punkt to\n",
                  "[nltk_data]     C:\\Users\\CasKei\\AppData\\Roaming\\nltk_data...\n",
                  "[nltk_data]   Package punkt is already up-to-date!\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "True"
                  ]
               },
               "execution_count": 10,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "from gensim.models.doc2vec import Doc2Vec,\\\n",
            "    TaggedDocument\n",
            "from nltk.tokenize import word_tokenize\n",
            "import nltk\n",
            "nltk.download('punkt')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [],
         "source": [
            "# preprocess to make a big list of sentences\n",
            "tagged_data = [TaggedDocument(words=word_tokenize(_d), tags=[str(i)]) for i, _d in enumerate(data['cleaned_text'])]\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Save the tokenized data as a pickle"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pickle\n",
            "\n",
            "# save tagged_data\n",
            "# with open('tagged_data.pickle', 'wb') as handle:\n",
            "#     pickle.dump(tagged_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
            "tagged_data = ''\n",
            "# open and load tags from pickle file\n",
            "with open('tagged_data.pickle', 'rb') as handle:\n",
            "    tagged_data = pickle.load(handle)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Doc2Vec training"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [],
         "source": [
            "# train the Doc2vec model\n",
            "model = Doc2Vec(vector_size=20,\n",
            "                min_count=2, epochs=10)\n",
            "model.build_vocab(tagged_data)\n",
            "model.train(tagged_data,\n",
            "            total_examples=model.corpus_count,\n",
            "            epochs=model.epochs)\n",
            " \n",
            "# get the document vectors\n",
            "document_vectors = [model.infer_vector(\n",
            "    word_tokenize(doc)) for doc in data['cleaned_text']]\n",
            " \n",
            "# #  print the document vectors\n",
            "# for i, doc in enumerate(cleaned):\n",
            "#     print(\"Document\", i+1, \":\", doc)\n",
            "#     print(\"Vector:\", document_vectors[i])\n",
            "#     print()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Save trained document vectors"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "metadata": {},
         "outputs": [],
         "source": [
            "# save\n",
            "with open('LR_document_vectors.pickle', 'wb') as handle:\n",
            "    pickle.dump(document_vectors, handle, protocol=pickle.HIGHEST_PROTOCOL)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "857934\n",
                  "857934\n"
               ]
            }
         ],
         "source": [
            "print(len(document_vectors))\n",
            "print(len(data))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "base",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.5"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
